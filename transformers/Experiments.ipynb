{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  PyTorch imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, PreTrainedTokenizer\n",
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "\n",
    "# Types\n",
    "from typing import List, Dict, Tuple, Union\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Khatchig Mouradian. Khatchig Mouradian is a jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jacob Henry Studer. Jacob Henry Studer (26 Feb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Stephen. Born in Glasgow, Stephen became ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Georgina Willis. Georgina Willis is an award w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stanley Corrsin. Corrsin was born on 3 April 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Khatchig Mouradian. Khatchig Mouradian is a jo...\n",
       "1  Jacob Henry Studer. Jacob Henry Studer (26 Feb...\n",
       "2  John Stephen. Born in Glasgow, Stephen became ...\n",
       "3  Georgina Willis. Georgina Willis is an award w...\n",
       "4  Stanley Corrsin. Corrsin was born on 3 April 1..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = []\n",
    "with open('data/wiki.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        wiki.append(line)\n",
    "\n",
    "wiki = pd.DataFrame(wiki, columns=['text'])\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Where was Khatchig Mouradian born?</th>\n",
       "      <th>Lebanon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where was Jacob Henry Studer born?</td>\n",
       "      <td>Columbus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where was John Stephen born?</td>\n",
       "      <td>Glasgow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where was Georgina Willis born?</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where was Stanley Corrsin born?</td>\n",
       "      <td>Philadelphia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where was Eduard Ender born?</td>\n",
       "      <td>Rome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Where was Khatchig Mouradian born?       Lebanon\n",
       "0  Where was Jacob Henry Studer born?      Columbus\n",
       "1        Where was John Stephen born?       Glasgow\n",
       "2     Where was Georgina Willis born?     Australia\n",
       "3     Where was Stanley Corrsin born?  Philadelphia\n",
       "4        Where was Eduard Ender born?          Rome"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read tsv file\n",
    "birth_places_train = pd.read_csv('data/birth_places_train.tsv', sep='\\t') \n",
    "birth_places_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Where was Bryan Dubreuiel born?</th>\n",
       "      <th>Atlanta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where was Ralf Wadephul born?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where was Joseph Baggaley born?</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where was Sandhya Sanjana born?</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where was Alfred Mele born?</td>\n",
       "      <td>Detroit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where was Murray Esler born?</td>\n",
       "      <td>Geelong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Where was Bryan Dubreuiel born?  Atlanta\n",
       "0    Where was Ralf Wadephul born?   Berlin\n",
       "1  Where was Joseph Baggaley born?  England\n",
       "2  Where was Sandhya Sanjana born?   Mumbai\n",
       "3      Where was Alfred Mele born?  Detroit\n",
       "4     Where was Murray Esler born?  Geelong"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birth_places_test = pd.read_csv('data/birth_places_test.tsv', sep='\\t')\n",
    "birth_places_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a single file dataset with $(q, c, a)$ structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_file(\n",
    "    qa_df: pd.DataFrame, # questions and answers\n",
    "    wiki_df: pd.DataFrame, # context\n",
    "    filename: str # name of the file to be created\n",
    ") -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Build dataset file for T5 training\n",
    "    \"\"\"\n",
    "    qa_df_values = qa_df.values\n",
    "    wiki_df_values = wiki_df.values\n",
    "\n",
    "    matches = {}\n",
    "    for i in tqdm(range(len(qa_df_values)), desc='Matching questions and contexts', total=len(qa_df_values)):\n",
    "        [q, a] = qa_df_values[i]\n",
    "        person = q.split(' ')[2:-1] # Get the name of the person\n",
    "        person = ' '.join(person) # Join the name of the person\n",
    "\n",
    "        # Find the context of the person\n",
    "        for j in range(len(wiki_df_values)):\n",
    "            c = wiki_df_values[j][0] # There is only one column in the wiki DataFrame\n",
    "            if person in c:\n",
    "                # Remove new line characters from context\n",
    "                c = c.replace('\\n', '')\n",
    "                matches[person] = (q, c, a)\n",
    "                break\n",
    "    \n",
    "    print(f'Number of entries preprocessed: {len(matches.keys())}')\n",
    "\n",
    "    # Build the dataset file\n",
    "    # Create a new csv file with the columns: question, context, answer\n",
    "    with open(f'data/{filename}.tsv', 'w', encoding='utf8') as f:\n",
    "        for k in matches.keys():\n",
    "            q, c, a = matches[k]\n",
    "            # Write the question, context and answer in the file on the same line\n",
    "            f.write(f'{q}\\t{c}\\t{a}\\n')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching questions and contexts: 100%|██████████| 1999/1999 [00:00<00:00, 2966.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries preprocessed: 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching questions and contexts: 100%|██████████| 499/499 [00:00<00:00, 1261.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries preprocessed: 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "build_dataset_file(birth_places_train, wiki, 'birth_places_train_clean')\n",
    "build_dataset_file(birth_places_test, wiki, 'birth_places_test_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where was Jacob Henry Studer born?</td>\n",
       "      <td>Jacob Henry Studer. Jacob Henry Studer (26 Feb...</td>\n",
       "      <td>Columbus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where was John Stephen born?</td>\n",
       "      <td>John Stephen. Born in Glasgow, Stephen became ...</td>\n",
       "      <td>Glasgow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where was Georgina Willis born?</td>\n",
       "      <td>Georgina Willis. Georgina Willis is an award w...</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where was Stanley Corrsin born?</td>\n",
       "      <td>Stanley Corrsin. Corrsin was born on 3 April 1...</td>\n",
       "      <td>Philadelphia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where was Eduard Ender born?</td>\n",
       "      <td>Eduard Ender. Eduard Ender (3 March 1822 Rome ...</td>\n",
       "      <td>Rome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             question  \\\n",
       "0  Where was Jacob Henry Studer born?   \n",
       "1        Where was John Stephen born?   \n",
       "2     Where was Georgina Willis born?   \n",
       "3     Where was Stanley Corrsin born?   \n",
       "4        Where was Eduard Ender born?   \n",
       "\n",
       "                                             context        answer  \n",
       "0  Jacob Henry Studer. Jacob Henry Studer (26 Feb...      Columbus  \n",
       "1  John Stephen. Born in Glasgow, Stephen became ...       Glasgow  \n",
       "2  Georgina Willis. Georgina Willis is an award w...     Australia  \n",
       "3  Stanley Corrsin. Corrsin was born on 3 April 1...  Philadelphia  \n",
       "4  Eduard Ender. Eduard Ender (3 March 1822 Rome ...          Rome  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv file\n",
    "train_dataset_df = pd.read_csv('data/birth_places_train_clean.tsv', sep='\\t', names=['question', 'context', 'answer'])\n",
    "train_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where was Ralf Wadephul born?</td>\n",
       "      <td>Ralf Wadephul. Ralf Wadephul, born 1958 in Ber...</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where was Joseph Baggaley born?</td>\n",
       "      <td>Joseph Baggaley. Joseph Baggaley (c. 1884 -- 1...</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where was Sandhya Sanjana born?</td>\n",
       "      <td>Sandhya Sanjana. Sandhya Sanjana (); is a sing...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where was Alfred Mele born?</td>\n",
       "      <td>Alfred Mele. Born in Detroit, Michigan, Mele a...</td>\n",
       "      <td>Detroit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where was Murray Esler born?</td>\n",
       "      <td>Murray Esler. Professor Murray David Esler (bo...</td>\n",
       "      <td>Geelong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          question  \\\n",
       "0    Where was Ralf Wadephul born?   \n",
       "1  Where was Joseph Baggaley born?   \n",
       "2  Where was Sandhya Sanjana born?   \n",
       "3      Where was Alfred Mele born?   \n",
       "4     Where was Murray Esler born?   \n",
       "\n",
       "                                             context   answer  \n",
       "0  Ralf Wadephul. Ralf Wadephul, born 1958 in Ber...   Berlin  \n",
       "1  Joseph Baggaley. Joseph Baggaley (c. 1884 -- 1...  England  \n",
       "2  Sandhya Sanjana. Sandhya Sanjana (); is a sing...   Mumbai  \n",
       "3  Alfred Mele. Born in Detroit, Michigan, Mele a...  Detroit  \n",
       "4  Murray Esler. Professor Murray David Esler (bo...  Geelong  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_df = pd.read_csv('data/birth_places_test_clean.tsv', sep='\\t', names=['question', 'context', 'answer'])\n",
    "test_dataset_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create utility functions for the $(q, c, a)$ scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the examples in input and target text format and the eos token at the end \n",
    "def add_eos_to_examples(entry: Tuple[str, str, str]) -> Tuple[str, str]:\n",
    "    question, context, answer = entry\n",
    "    result = {}\n",
    "    result['input_text'] = 'question: %s  context: %s </s>' % (question, context)\n",
    "    result['target_text'] = '%s </s>' % (answer)\n",
    "    return result\n",
    "\n",
    "# tokenize the examples\n",
    "def convert_to_features(qa_entry: Tuple[str, str, str], tokenizer: PreTrainedTokenizer) -> Dict[str, List[int]]:\n",
    "    input_encodings = tokenizer.encode_plus(text=qa_entry['input_text'], padding='max_length', max_length=512, truncation=True) # the context is limited to 512 tokens\n",
    "    target_encodings = tokenizer.encode_plus(text=qa_entry['target_text'], padding='max_length', max_length=32, truncation=True) # the answer is limited to 32 tokens, which should conver most of the answers \n",
    "\n",
    "    encodings = {\n",
    "        'input_ids': np.array(input_encodings['input_ids']), \n",
    "        'attention_mask': np.array(input_encodings['attention_mask']),\n",
    "        'target_ids': np.array(target_encodings['input_ids']),\n",
    "        'target_attention_mask': np.array(target_encodings['attention_mask'])\n",
    "    }\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test the functions on a sample\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "sample = train_dataset_df.sample(1)\n",
    "sample = add_eos_to_examples(sample.values[0])\n",
    "sample = convert_to_features(sample, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirthPlaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for birth place prediction made on a (q, c, a) format,\n",
    "    where q is the question, c is the context and a is the answer.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the questions, contexts and answers\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "        self.df_values = self.df.values\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, List[int]]:\n",
    "        entry = self.df_values[idx]\n",
    "        entry = add_eos_to_examples(entry)\n",
    "        entry = convert_to_features(entry, self.tokenizer)\n",
    "        return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two datasets, one for training and one for validation\n",
    "train_dataset = BirthPlaceDataset(train_dataset_df)\n",
    "val_dataset = BirthPlaceDataset(test_dataset_df)\n",
    "\n",
    "# Check if each element in the list of batches has the same shape\n",
    "\n",
    "# Check for input_ids\n",
    "for i in range(len(val_dataset)):\n",
    "    entry = train_dataset[i]\n",
    "    if entry['input_ids'].shape != (512,):\n",
    "        print(f'Entry {i} has a different shape for input_ids: {entry[\"input_ids\"].shape}')\n",
    "\n",
    "# Check for attention_mask\n",
    "for i in range(len(val_dataset)):\n",
    "    entry = train_dataset[i]\n",
    "    if entry['attention_mask'].shape != (512,):\n",
    "        print(f'Entry {i} has a different shape for attention_mask: {entry[\"attention_mask\"].shape}')\n",
    "\n",
    "# Check for target_ids\n",
    "for i in range(len(val_dataset)):\n",
    "    entry = train_dataset[i]\n",
    "    if entry['target_ids'].shape != (32,):\n",
    "        print(f'Entry {i} has a different shape for target_ids: {entry[\"target_ids\"].shape}')\n",
    "\n",
    "# Check for target_attention_mask\n",
    "for i in range(len(val_dataset)):\n",
    "    entry = train_dataset[i]\n",
    "    if entry['target_attention_mask'].shape != (32,):\n",
    "        print(f'Entry {i} has a different shape for target_attention_mask: {entry[\"target_attention_mask\"].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A sample from new the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1990\n",
      "Number of validation examples: 499\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_dataset)}')\n",
    "print(f'Number of validation examples: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load t5 small model for question answering\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', return_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the t5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie alt sind Sie?\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "input_ids = tokenizer.encode(\"translate English to German: How old are you?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids, max_length=40, num_beams=4, early_stopping=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ Task 1 ] Cold evaluation of the pretrained T5-small model\n",
    "\n",
    "The evaluation script was taken from SQuAD dataset and it validates the exact matches of the model answers compared with the real ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Test is each batch has the correct shape\n",
    "for batch in train_dataloader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    target_ids = batch['target_ids'].to(device)\n",
    "    target_attention_mask = batch['target_attention_mask'].to(device)\n",
    "    assert input_ids.shape == (8, 512), f'input_ids has a shape of {input_ids.shape}'\n",
    "    assert attention_mask.shape == (8, 512), f'attention_mask has a shape of {attention_mask.shape}'\n",
    "    assert target_ids.shape == (8, 32), f'target_ids has a shape of {target_ids.shape}'\n",
    "    assert target_attention_mask.shape == (8, 32), f'target_attention_mask has a shape of {target_attention_mask.shape}'\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(gold_answers, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "\n",
    "    for ground_truths, prediction in zip(gold_answers, predictions):\n",
    "      total += 1\n",
    "      exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "      f1 += metric_max_over_ground_truths(\n",
    "          f1_score, prediction, ground_truths)\n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [03:32<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "  outs = model.generate(input_ids=batch['input_ids'], \n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        max_length=32,\n",
    "                        early_stopping=True)\n",
    "  outs = [tokenizer.decode(ids) for ids in outs]\n",
    "  answers.extend(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "for ref, pred in zip(val_dataset, answers):\n",
    "  a = ref['target_ids']  \n",
    "  a = tokenizer.decode(a)\n",
    "\n",
    "  # Remove padding\n",
    "  a = a.replace('<pad>', '')\n",
    "  a = a.replace('</s>', '')\n",
    "  a = a.replace('<s>', '')\n",
    "  \n",
    "  # Remove all whitespace from the beginning and end\n",
    "  a = a.rstrip()\n",
    "\n",
    "  pred = pred.replace('<pad>', '')\n",
    "  pred = pred.replace('</s>', '')\n",
    "  pred = pred.replace('<s>', '')\n",
    "  \n",
    "  # Remove all whitespace\n",
    "  pred = pred.rstrip()\n",
    "\n",
    "  # Remove the space at the beginning of all predictions\n",
    "  if pred[0] == ' ': \n",
    "    pred = pred[1:]\n",
    "\n",
    "  predictions.append(pred)\n",
    "  references.append(a)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Oxford, England', 'Washington')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[11], references[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0, 'f1': 0.10020040080160321}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(references, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion on preliminary results\n",
    "\n",
    "TBD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Task 2] Finetuning process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are two different possibilities:\n",
    "1. Training with the HuggingFace API (i.e Trainer)\n",
    "2. Training in the classical manner or in our case the PyTorch way\n",
    "\n",
    "For this model, in order to have better tracktability I will use the second option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "You need to use GPU for this notebook, reset the runtime and select GPU",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[39m# Check if the device is GPU ~ only for colab\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39massert\u001b[39;00m device \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mYou need to use GPU for this notebook, reset the runtime and select GPU\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: You need to use GPU for this notebook, reset the runtime and select GPU"
     ]
    }
   ],
   "source": [
    "# Create the new training environment\n",
    "finetune_model = model # use the same model as before\n",
    "finetune_tokenizer = tokenizer # use the same tokenizer as before\n",
    "\n",
    "# Create the new optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Data environment\n",
    "finetune_train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "finetune_val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Check if the device is GPU ~ only for colab\n",
    "assert device == 'cuda', 'You need to use GPU for this notebook, reset the runtime and select GPU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free cuda memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m target_attention_mask \u001b[39m=\u001b[39m target_attention_mask\u001b[39m.\u001b[39mlong()\n\u001b[0;32m     18\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m outputs \u001b[39m=\u001b[39m finetune_model(input_ids\u001b[39m=\u001b[39;49minput_ids, \n\u001b[0;32m     20\u001b[0m                          attention_mask\u001b[39m=\u001b[39;49mattention_mask, \n\u001b[0;32m     21\u001b[0m                          decoder_input_ids\u001b[39m=\u001b[39;49mtarget_ids, \n\u001b[0;32m     22\u001b[0m                          decoder_attention_mask\u001b[39m=\u001b[39;49mtarget_attention_mask,\n\u001b[0;32m     23\u001b[0m                          labels\u001b[39m=\u001b[39;49mtarget_ids)\n\u001b[0;32m     25\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[0;32m     26\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1648\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[0;32m   1647\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m-> 1648\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   1649\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1650\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1651\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1652\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1653\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m   1654\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1655\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1656\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1657\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1658\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1659\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1660\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1661\u001b[0m )\n\u001b[0;32m   1663\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1665\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1040\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   1028\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1029\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1039\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1040\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   1041\u001b[0m         hidden_states,\n\u001b[0;32m   1042\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1043\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m   1044\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1045\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1046\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[0;32m   1047\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m   1048\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m   1049\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   1050\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1051\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1052\u001b[0m     )\n\u001b[0;32m   1054\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:673\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m0\u001b[39;49m](\n\u001b[0;32m    674\u001b[0m     hidden_states,\n\u001b[0;32m    675\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    676\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    677\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    678\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    679\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    680\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    681\u001b[0m )\n\u001b[0;32m    682\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    683\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:579\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    569\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    570\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    577\u001b[0m ):\n\u001b[0;32m    578\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 579\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSelfAttention(\n\u001b[0;32m    580\u001b[0m         normed_hidden_states,\n\u001b[0;32m    581\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    582\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    583\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    584\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    585\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    586\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    587\u001b[0m     )\n\u001b[0;32m    588\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[0;32m    589\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:542\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    538\u001b[0m scores \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_bias_masked\n\u001b[0;32m    539\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(scores\u001b[39m.\u001b[39mfloat(), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtype_as(\n\u001b[0;32m    540\u001b[0m     scores\n\u001b[0;32m    541\u001b[0m )  \u001b[39m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mdropout(\n\u001b[0;32m    543\u001b[0m     attn_weights, p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining\n\u001b[0;32m    544\u001b[0m )  \u001b[39m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[39mif\u001b[39;00m layer_head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Popescu Andrei\\anaconda3\\envs\\playground\\lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    finetune_model.to(device)\n",
    "    finetune_model.train()\n",
    "    epoch_loss_values = []\n",
    "\n",
    "    for batch in tqdm(finetune_train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        target_attention_mask = batch['target_attention_mask'].to(device)\n",
    "        \n",
    "        # Convert tensors from type Int to Long\n",
    "        input_ids = input_ids.long()\n",
    "        attention_mask = attention_mask.long()\n",
    "        target_ids = target_ids.long()\n",
    "        target_attention_mask = target_attention_mask.long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = finetune_model(input_ids=input_ids, \n",
    "                                 attention_mask=attention_mask, \n",
    "                                 decoder_input_ids=target_ids, \n",
    "                                 decoder_attention_mask=target_attention_mask,\n",
    "                                 labels=target_ids)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = outputs[0]\n",
    "        epoch_loss_values.append(loss.item())\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        del loss\n",
    "    \n",
    "    # Print a beutiful format of the loss\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save the model\n",
    "finetune_model.to('cpu')\n",
    "finetune_model.save_pretrained('models/finetune_model_{}.pt'.format(epoch+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "finetune_model.eval()\n",
    "finetune_model.to(device)\n",
    "\n",
    "answers = []\n",
    "for batch in tqdm(finetune_val_dataloader):\n",
    "    outs = finetune_model.generate(input_ids=batch['input_ids'], \n",
    "                            attention_mask=batch['attention_mask'],\n",
    "                            max_length=32,\n",
    "                            early_stopping=True)\n",
    "    outs = [tokenizer.decode(ids) for ids in outs]\n",
    "    answers.extend(outs)\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "for ref, pred in zip(val_dataset, answers):\n",
    "    a = ref['target_ids']  \n",
    "    a = tokenizer.decode(a)\n",
    "\n",
    "    # Remove padding\n",
    "    a = a.replace('<pad>', '')\n",
    "    a = a.replace('</s>', '')\n",
    "    a = a.replace('<s>', '')\n",
    "    \n",
    "    # Remove all whitespace from the beginning and end\n",
    "    a = a.rstrip()\n",
    "\n",
    "    pred = pred.replace('<pad>', '')\n",
    "    pred = pred.replace('</s>', '')\n",
    "    pred = pred.replace('<s>', '')\n",
    "    \n",
    "    # Remove all whitespace\n",
    "    pred.replace(' ', '')\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(a)\n",
    "\n",
    "evaluate(references, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbfaf70ce1e237e4fd4612c2cbc8fa3a23d8e9214fa363005031f0c99ae028bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
