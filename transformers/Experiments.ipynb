{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  PyTorch imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, PreTrainedTokenizer\n",
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "\n",
    "# Types\n",
    "from typing import List, Dict, Tuple, Union\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = []\n",
    "with open('data/wiki.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        wiki.append(line)\n",
    "\n",
    "wiki = pd.DataFrame(wiki, columns=['text'])\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tsv file\n",
    "birth_places_train = pd.read_csv('data/birth_places_train.tsv', sep='\\t') \n",
    "birth_places_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_places_test = pd.read_csv('data/birth_places_test.tsv', sep='\\t')\n",
    "birth_places_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a single file dataset with $(q, c, a)$ structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_file(\n",
    "    qa_df: pd.DataFrame, # questions and answers\n",
    "    wiki_df: pd.DataFrame, # context\n",
    "    filename: str # name of the file to be created\n",
    ") -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Build dataset file for T5 training\n",
    "    \"\"\"\n",
    "    qa_df_values = qa_df.values\n",
    "    wiki_df_values = wiki_df.values\n",
    "\n",
    "    matches = {}\n",
    "    for i in tqdm(range(len(qa_df_values)), desc='Matching questions and contexts', total=len(qa_df_values)):\n",
    "        [q, a] = qa_df_values[i]\n",
    "        person = q.split(' ')[2:-1] # Get the name of the person\n",
    "        person = ' '.join(person) # Join the name of the person\n",
    "\n",
    "        # Find the context of the person\n",
    "        for j in range(len(wiki_df_values)):\n",
    "            c = wiki_df_values[j][0] # There is only one column in the wiki DataFrame\n",
    "            if person in c:\n",
    "                # Remove new line characters from context\n",
    "                c = c.replace('\\n', '')\n",
    "                matches[person] = (q, c, a)\n",
    "                break\n",
    "    \n",
    "    print(f'Number of entries preprocessed: {len(matches.keys())}')\n",
    "\n",
    "    # Build the dataset file\n",
    "    # Create a new csv file with the columns: question, context, answer\n",
    "    with open(f'data/{filename}.tsv', 'w', encoding='utf8') as f:\n",
    "        for k in matches.keys():\n",
    "            q, c, a = matches[k]\n",
    "            # Write the question, context and answer in the file on the same line\n",
    "            f.write(f'{q}\\t{c}\\t{a}\\n')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dataset_file(birth_places_train, wiki, 'birth_places_train_clean')\n",
    "build_dataset_file(birth_places_test, wiki, 'birth_places_test_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "train_dataset_df = pd.read_csv('data/birth_places_train_clean.tsv', sep='\\t', names=['question', 'context', 'answer'])\n",
    "train_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_df = pd.read_csv('data/birth_places_test_clean.tsv', sep='\\t', names=['question', 'context', 'answer'])\n",
    "test_dataset_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create utility functions for the $(q, c, a)$ scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the examples in input and target text format and the eos token at the end \n",
    "def add_eos_to_examples(entry: Tuple[str, str, str]) -> Tuple[str, str]:\n",
    "    question, context, answer = entry\n",
    "    result = {}\n",
    "    result['input_text'] = 'question: %s  context: %s </s>' % (question, context)\n",
    "    result['target_text'] = '%s </s>' % (answer)\n",
    "    return result\n",
    "\n",
    "# tokenize the examples\n",
    "def convert_to_features(qa_entry: Tuple[str, str, str], tokenizer: PreTrainedTokenizer) -> Dict[str, List[int]]:\n",
    "    input_encodings = tokenizer.encode_plus(text=qa_entry['input_text'], padding='max_length', max_length=512, truncation=True) # the context is limited to 512 tokens\n",
    "    target_encodings = tokenizer.encode_plus(text=qa_entry['target_text'], padding='max_length', max_length=32, truncation=True) # the answer is limited to 32 tokens, which should conver most of the answers \n",
    "\n",
    "    encodings = {\n",
    "        'input_ids': np.array(input_encodings['input_ids']), \n",
    "        'attention_mask': np.array(input_encodings['attention_mask']),\n",
    "        'target_ids': np.array(target_encodings['input_ids']),\n",
    "        'target_attention_mask': np.array(target_encodings['attention_mask'])\n",
    "    }\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functions on a sample\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "sample = train_dataset_df.sample(1)\n",
    "sample = add_eos_to_examples(sample.values[0])\n",
    "sample = convert_to_features(sample, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirthPlaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for birth place prediction made on a (q, c, a) format,\n",
    "    where q is the question, c is the context and a is the answer.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the questions, contexts and answers\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "        self.df_values = self.df.values\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, List[int]]:\n",
    "        entry = self.df_values[idx]\n",
    "        entry = add_eos_to_examples(entry)\n",
    "        entry = convert_to_features(entry, self.tokenizer)\n",
    "        return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two datasets, one for training and one for validation\n",
    "train_dataset = BirthPlaceDataset(train_dataset_df)\n",
    "val_dataset = BirthPlaceDataset(test_dataset_df)\n",
    "\n",
    "# Check if each element in the list of batches has the same shape\n",
    "\n",
    "# Check for input_ids\n",
    "for i in range(len(val_dataset)):\n",
    "    entry = train_dataset[i]\n",
    "    if entry['input_ids'].shape != (512,):\n",
    "        print(f'Entry {i} has a different shape for input_ids: {entry[\"input_ids\"].shape}')\n",
    "\n",
    "# Check for attention_mask\n",
    "for i in range(len(val_dataset)):\n",
    "    entry = train_dataset[i]\n",
    "    if entry['attention_mask'].shape != (512,):\n",
    "        print(f'Entry {i} has a different shape for attention_mask: {entry[\"attention_mask\"].shape}')\n",
    "\n",
    "# Check for target_ids\n",
    "for i in range(len(val_dataset)):\n",
    "    entry = train_dataset[i]\n",
    "    if entry['target_ids'].shape != (32,):\n",
    "        print(f'Entry {i} has a different shape for target_ids: {entry[\"target_ids\"].shape}')\n",
    "\n",
    "# Check for target_attention_mask\n",
    "for i in range(len(val_dataset)):\n",
    "    entry = train_dataset[i]\n",
    "    if entry['target_attention_mask'].shape != (32,):\n",
    "        print(f'Entry {i} has a different shape for target_attention_mask: {entry[\"target_attention_mask\"].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A sample from new the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_dataset)}')\n",
    "print(f'Number of validation examples: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load t5 small model for question answering\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', return_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the t5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "input_ids = tokenizer.encode(\"translate English to German: How old are you?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids, max_length=40, num_beams=4, early_stopping=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ Task 1 ] Cold evaluation of the pretrained T5-small model\n",
    "\n",
    "The evaluation script was taken from SQuAD dataset and it validates the exact matches of the model answers compared with the real ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Test is each batch has the correct shape\n",
    "for batch in train_dataloader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    target_ids = batch['target_ids'].to(device)\n",
    "    target_attention_mask = batch['target_attention_mask'].to(device)\n",
    "    assert input_ids.shape == (8, 512), f'input_ids has a shape of {input_ids.shape}'\n",
    "    assert attention_mask.shape == (8, 512), f'attention_mask has a shape of {attention_mask.shape}'\n",
    "    assert target_ids.shape == (8, 32), f'target_ids has a shape of {target_ids.shape}'\n",
    "    assert target_attention_mask.shape == (8, 32), f'target_attention_mask has a shape of {target_attention_mask.shape}'\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(gold_answers, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "\n",
    "    for ground_truths, prediction in zip(gold_answers, predictions):\n",
    "      total += 1\n",
    "      exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "      f1 += metric_max_over_ground_truths(\n",
    "          f1_score, prediction, ground_truths)\n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "  outs = model.generate(input_ids=batch['input_ids'], \n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        max_length=32,\n",
    "                        early_stopping=True)\n",
    "  outs = [tokenizer.decode(ids) for ids in outs]\n",
    "  answers.extend(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "for ref, pred in zip(val_dataset, answers):\n",
    "  a = ref['target_ids']  \n",
    "  a = tokenizer.decode(a)\n",
    "\n",
    "  # Remove padding\n",
    "  a = a.replace('<pad>', '')\n",
    "  a = a.replace('</s>', '')\n",
    "  a = a.replace('<s>', '')\n",
    "  \n",
    "  # Remove all whitespace from the beginning and end\n",
    "  a = a.rstrip()\n",
    "\n",
    "  pred = pred.replace('<pad>', '')\n",
    "  pred = pred.replace('</s>', '')\n",
    "  pred = pred.replace('<s>', '')\n",
    "  \n",
    "  # Remove all whitespace\n",
    "  pred = pred.rstrip()\n",
    "\n",
    "  # Remove the space at the beginning of all predictions\n",
    "  if pred[0] == ' ': \n",
    "    pred = pred[1:]\n",
    "\n",
    "  predictions.append(pred)\n",
    "  references.append(a)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[11], references[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(references, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion on preliminary results\n",
    "\n",
    "```bash\n",
    "{'exact_match': 52.67, 'f1': 0.512}\n",
    "```\n",
    "\n",
    "This prelimiary result indicate the fact that the model is not capable to indicate the qood answers, or not more than a random selection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Task 2] Finetuning process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are two different possibilities:\n",
    "1. Training with the HuggingFace API (i.e Trainer)\n",
    "2. Training in the classical manner or in our case the PyTorch way\n",
    "\n",
    "For this model, in order to have better tracktability I will use the second option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new training environment\n",
    "finetune_model = model # use the same model as before\n",
    "finetune_tokenizer = tokenizer # use the same tokenizer as before\n",
    "\n",
    "# Create the new optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Data environment\n",
    "finetune_train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "finetune_val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Check if the device is GPU ~ only for colab\n",
    "assert device == 'cuda', 'You need to use GPU for this notebook, reset the runtime and select GPU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free cuda memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    finetune_model.to(device)\n",
    "    finetune_model.train()\n",
    "    epoch_loss_values = []\n",
    "\n",
    "    for batch in tqdm(finetune_train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        target_attention_mask = batch['target_attention_mask'].to(device)\n",
    "        \n",
    "        # Convert tensors from type Int to Long\n",
    "        input_ids = input_ids.long()\n",
    "        attention_mask = attention_mask.long()\n",
    "        target_ids = target_ids.long()\n",
    "        target_attention_mask = target_attention_mask.long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = finetune_model(input_ids=input_ids, \n",
    "                                 attention_mask=attention_mask, \n",
    "                                 decoder_input_ids=target_ids, \n",
    "                                 decoder_attention_mask=target_attention_mask,\n",
    "                                 labels=target_ids)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = outputs[0]\n",
    "        epoch_loss_values.append(loss.item())\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        del loss\n",
    "    \n",
    "    # Print a beutiful format of the loss\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save the model\n",
    "finetune_model.to('cpu')\n",
    "finetune_model.save_pretrained('models/finetune_model_{}.pt'.format(epoch+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "finetune_model.eval()\n",
    "finetune_model.to(device)\n",
    "\n",
    "answers = []\n",
    "for batch in tqdm(finetune_val_dataloader):\n",
    "    outs = finetune_model.generate(input_ids=batch['input_ids'], \n",
    "                            attention_mask=batch['attention_mask'],\n",
    "                            max_length=32,\n",
    "                            early_stopping=True)\n",
    "    outs = [tokenizer.decode(ids) for ids in outs]\n",
    "    answers.extend(outs)\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "for ref, pred in zip(val_dataset, answers):\n",
    "    a = ref['target_ids']  \n",
    "    a = tokenizer.decode(a)\n",
    "\n",
    "    # Remove padding\n",
    "    a = a.replace('<pad>', '')\n",
    "    a = a.replace('</s>', '')\n",
    "    a = a.replace('<s>', '')\n",
    "    \n",
    "    # Remove all whitespace from the beginning and end\n",
    "    a = a.rstrip()\n",
    "\n",
    "    pred = pred.replace('<pad>', '')\n",
    "    pred = pred.replace('</s>', '')\n",
    "    pred = pred.replace('<s>', '')\n",
    "    \n",
    "    # Remove all whitespace\n",
    "    pred.replace(' ', '')\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(a)\n",
    "\n",
    "evaluate(references, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Final results\n",
    "\n",
    "```bash\n",
    "Final results: {'exact_match': 91.703, 'f1': 0.90256}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbfaf70ce1e237e4fd4612c2cbc8fa3a23d8e9214fa363005031f0c99ae028bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
