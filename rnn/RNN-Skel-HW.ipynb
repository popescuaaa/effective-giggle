{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfIdnkh92358"
   },
   "source": [
    "# NN HW 2 - Simple RNN training \n",
    "### Adapted from the EEML2019 Tutorial on RNNs\n",
    "\n",
    "The objective is to analyze the training of various RNNs on simple datasets and doing some analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MA_K3_OL3EY4"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "j5YGV2hb2RIt"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "  \n",
    "sns.set_style('ticks')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0KPZdiq5AVJ"
   },
   "source": [
    "# Vanilla RNN\n",
    "\n",
    "Implement basic RNN cell using tf.layers.\n",
    "\n",
    "   $$ h_t = f( Wx_t + Vh_{t-1}  + b) $$\n",
    "   \n",
    "   Where\n",
    "   \n",
    "   * $x_t$ input at time $t$\n",
    "   * $h_t$ hidden state at time $t$\n",
    "   * $W$ input-to-hidden mapping (trainable)\n",
    "   * $V$ hidden-to-hidden mapping (trainable)\n",
    "   * $b$ bias (trainable)\n",
    "   * $f$ non-linearity chosen (usually tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Vanilla RNN Base recurrence model\n",
    "$h_t = f(input + V h_{(t-1)} + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "z6GIHgOnzd8Y"
   },
   "outputs": [],
   "source": [
    "class VanillaRNNBase(nn.Module):\n",
    "  \n",
    "    def __init__(self, hidden_size, activation=nn.Tanh, bias=True):    \n",
    "        \"\"\"\n",
    "        Constructor for a simple RNNCell where the hidden-to-hidden transitions\n",
    "        are defined by a linear layer and the default activation of `tanh` \n",
    "        :param hidden_size: the size of the hidden state\n",
    "        :param activation: the activation function used for computing the next hidden state\n",
    "        \"\"\"\n",
    "        super(VanillaRNNBase, self).__init__()\n",
    "    \n",
    "        self._hidden_size = hidden_size\n",
    "        self._activation = activation()  \n",
    "        self._bias = bias\n",
    "            \n",
    "        # TODO 1.1 Create the hidden-to-hidden layer\n",
    "        # self._linear_hh = nn.Linear(...)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        out = inputs\n",
    "        #### TODO 1.1 Your code here\n",
    "        ### ...\n",
    "        #### end code\n",
    "        \n",
    "        return out, out\n",
    "\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=20, bias=False):\n",
    "        \"\"\"\n",
    "        Creates a vanilla RNN where input-to-hidden is a nn.Linear layer\n",
    "        and hidden-to-output is a nn.Linear layer\n",
    "        \n",
    "        :param input_size: the size of the input to the RNN\n",
    "        :param hidden_size: size of the hidden state of the RNN\n",
    "        :param output_size: size of the output\n",
    "        \"\"\"\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "        self._bias = bias\n",
    "        \n",
    "        self.in_to_hidden = nn.Linear(self._input_size, self._hidden_size, bias=self._bias)\n",
    "        self.rnn_cell = VanillaRNNBase(self._hidden_size, bias=self._bias)\n",
    "        self.hidden_to_out = nn.Linear(self._hidden_size, self._output_size, bias=self._bias)\n",
    "    \n",
    "    def step(self, input, hidden=None):\n",
    "        ### TODO 1.2 compute one step in the RNN\n",
    "        ## input_ = ....\n",
    "        ## _, hidden_ =  ....\n",
    "        # output_ = ....\n",
    "        \n",
    "        # return output_, hidden_\n",
    "        pass\n",
    "    \n",
    "    def forward(self, inputs, hidden=None, force=True, warm_start=10):\n",
    "        steps = len(inputs)\n",
    "        \n",
    "        outputs = torch.autograd.Variable(torch.zeros(steps, self._output_size, self._output_size))\n",
    "        \n",
    "        output_ = None\n",
    "        hidden_ = hidden\n",
    "        \n",
    "        for i in range(steps):\n",
    "            ## TODO 1.3 Implement forward pass in RNN\n",
    "            ## Implement Teacher Forcing and Warm Start\n",
    "            input_ = None\n",
    "            \n",
    "            ### END Code\n",
    "            \n",
    "            output_, hidden_ = self.step(input_, hidden_)\n",
    "            outputs[i] = output_\n",
    "            \n",
    "        return outputs, hidden_\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jCR9YGaI7my"
   },
   "source": [
    "## Train RNN on sine wave\n",
    "\n",
    "Train the RNN on sine data - predict the next sine value from *predicted* sine values.\n",
    "\n",
    "Predict   $$ sin (x +t \\epsilon) $$ from $$ sin (x), sin (x + \\epsilon), ..., sin (x + (t-1) \\epsilon) $$\n",
    "\n",
    "In particular, we want the network to predict the next value in a loop, conditioning the prediction on some initial values (provided) and all subsequent predictions.\n",
    "\n",
    "To learn the prediction model, we will use *teacher forcing*. This means that when training the model, the input at time $t$ is the real sequence at time $t$, rather than the output produced by the model at $t-1$.\n",
    "\n",
    "When we want to generate data from the model, we do not have access to the true sequence, so we do not use teacher forcing. However, in the case of our problem, we will also use *warm starting*, because we require multiple time steps to predict the next sine wave value (at least 2, for the initial value and for the step). \n",
    "\n",
    "The code below unrolls the RNN core you have defined above, does the training using backprop though time and plots the real data (\"ground truth\"), the data generated during training (\"train predictions\") and the model samples \"generated\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f'Running code @ {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNROLL_LENGTH = 30  #@param {type:\"integer\"}\n",
    "NUM_ITERATIONS = 10000  #@param {type:\"integer\"}\n",
    "WARM_START = 10  #@param {type:\"integer\"}\n",
    "TEACHER_FORCING = False  #@param {type:\"boolean\"}\n",
    "HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
    "LEARNING_RATE = 0.0001  #@param {type:\"number\"}\n",
    "REPORTING_INTERVAL = 200  #@param {type:\"integer\"}\n",
    "\n",
    "# We create training data, sine wave over [0, 2pi]\n",
    "x_train = np.arange(0, 2*np.pi, 0.1).reshape(-1, 1, 1)\n",
    "y_train = np.sin(x_train)\n",
    "\n",
    "net = VanillaRNN(hidden_size=HIDDEN_UNITS, bias=False)\n",
    "net.train()\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    # select a start point in the training set for a sequence of UNROLL_LENGTH\n",
    "    start = np.random.choice(range(x_train.shape[0] - UNROLL_LENGTH))\n",
    "    train_sequence = y_train[start : (start + UNROLL_LENGTH)]\n",
    "    \n",
    "    train_inputs = torch.from_numpy(train_sequence[:-1]).float().to(device)\n",
    "    train_targets = torch.from_numpy(train_sequence[1:]).float().to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs, hidden = net(train_inputs, hidden=None, force=TEACHER_FORCING, warm_start=WARM_START)\n",
    "    loss = criterion(outputs, train_targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if iteration % REPORTING_INTERVAL == REPORTING_INTERVAL - 1:\n",
    "        # let's see how well we do on predictions for the whole sequence\n",
    "        avg_loss = running_loss / REPORTING_INTERVAL\n",
    "        \n",
    "        report_sequence = torch.from_numpy(y_train[:-1]).float().to(device)\n",
    "        report_targets = torch.from_numpy(y_train[1:]).float().to(device)\n",
    "        report_output, report_hidden = net(report_sequence, hidden=None, force=False, warm_start=WARM_START)\n",
    "        \n",
    "        report_loss = criterion(report_output, report_targets)\n",
    "        print('[%d] avg_loss: %.5f, report_loss: %.5f, ' % (iteration + 1, avg_loss, report_loss.item()))\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title('Training Loss %.5f;  Sampling loss %.5f; Iteration %d' % (avg_loss, report_loss.item(), iteration))\n",
    "        \n",
    "        plt.plot(y_train[1:].ravel(), c='blue', label='Ground truth',\n",
    "               linestyle=\":\", lw=6)\n",
    "        plt.plot(range(start, start+UNROLL_LENGTH-1), outputs.data.numpy().ravel(), c='gold',\n",
    "               label='Train prediction', lw=5, marker=\"o\", markersize=5,\n",
    "               alpha=0.7)\n",
    "        plt.plot(report_output.data.numpy().ravel(), c='r', label='Generated', lw=4, alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOKWVWE9sDke"
   },
   "outputs": [],
   "source": [
    "# Default hypers:\n",
    "# UNROLL_LENGTH = 30  #@param {type:\"integer\"}\n",
    "# NUM_ITERATIONS = 10000  #@param {type:\"integer\"}\n",
    "# WARM_START = 2  #@param {type:\"integer\"}\n",
    "# TEACHER_FORCING = False  #@param {type:\"boolean\"}\n",
    "# HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
    "# LEARNING_RATE = 0.0001  #@param {type:\"number\"}\n",
    "# REPORTING_INTERVAL = 2000  #@param {type:\"integer\"}\n",
    "\n",
    "# You may want to try:\n",
    "# default hypers with/without teacher forcing\n",
    "# use UNROLL_LENGTH = 62 to train on the whole sequence (is teacher forcing useful?)\n",
    "# use UNROLL_LENGTH = 62, no teacher forcing and warm_start = 2 # this should break training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLuIAK8LJWay"
   },
   "source": [
    "**Note:** initialization is not fixed (we do not fix a random seed), so each time the cell is executed, the parameters take new initial values and hence training can lead to different results. What happens if you run it multiple times?\n",
    "\n",
    "###What is worth trying/understanding here?\n",
    "\n",
    "* Difference between teacher forcing and learning on own samples:\n",
    " * What are the pros and cons of teacher forcing?\n",
    " * Why is the model struggling to learn in one of the setups?\n",
    " * What is it we actually care about for models like this? What should be the actual surrogate?\n",
    "* How does warm starting affect our training? Why?\n",
    "* What happens if the structure of interest is much longer than the unroll length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpXpLbLVobgN"
   },
   "source": [
    "# Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwojS8kx0QyE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EEML2019_RNN_full.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
